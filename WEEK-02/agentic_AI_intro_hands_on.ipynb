{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivaris/AIML-GENAI/blob/main/WEEK-02/agentic_AI_intro_hands_on.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17cb001a"
      },
      "source": [
        "# Agentic AI Demo Notebook\n",
        "\n",
        "This notebook introduces Agentic AI by building a series of simply Agents using LangChain.\n",
        "\n",
        "**Flow:**\n",
        "1. Environment setup & API keys  \n",
        "2. Tools (e.g., email/web search) & why agents need tools  \n",
        "3. LLM and agent setup (ReAct)  \n",
        "4. Running the agent & inspecting messages  \n",
        "5. Memory  \n",
        "6. MCP demo (Model Context Protocol) & dynamic tool discovery  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38301ee5"
      },
      "source": [
        "## Environment Setup & Configurations\n",
        "Install required libraries for LangChain/LangGraph/MCP and supporting packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain-openai langgraph ddgs langchain-core langchain-experimental langchain-mcp langchain.tools langchain-mcp-adapters nest_asyncio"
      ],
      "metadata": {
        "id": "XEjDsW-pCuur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d562b016-6e6b-474d-b0a6-61d116523050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting ddgs\n",
            "  Downloading ddgs-9.5.4-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.74)\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langchain-mcp\n",
            "  Downloading langchain_mcp-0.2.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langchain.tools\n",
            "  Downloading langchain_tools-0.1.34-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting langchain-mcp-adapters\n",
            "  Downloading langchain_mcp_adapters-0.1.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.100.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from ddgs)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting lxml>=6.0.0 (from ddgs)\n",
            "  Downloading lxml-6.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental)\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: mcp~=1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-mcp) (1.13.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from langchain.tools) (0.3.27)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain->langchain.tools) (0.3.9)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (0.24.0)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp~=1.0->langchain-mcp) (4.10.0)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp~=1.0->langchain-mcp) (4.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp~=1.0->langchain-mcp) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp~=1.0->langchain-mcp) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp~=1.0->langchain-mcp) (0.47.2)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp~=1.0->langchain-mcp) (0.35.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp~=1.0->langchain-mcp) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp) (0.27.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ddgs-9.5.4-py3-none-any.whl (37 kB)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_mcp-0.2.1-py3-none-any.whl (4.1 kB)\n",
            "Downloading langchain_tools-0.1.34-py3-none-any.whl (8.5 kB)\n",
            "Downloading langchain_mcp_adapters-0.1.9-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.2-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-6.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: primp, ormsgpack, mypy-extensions, marshmallow, lxml, typing-inspect, ddgs, langgraph-sdk, dataclasses-json, langgraph-checkpoint, langchain-openai, langchain-mcp-adapters, langchain-mcp, langgraph-prebuilt, langgraph, langchain-community, langchain.tools, langchain-experimental\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "Successfully installed dataclasses-json-0.6.7 ddgs-9.5.4 langchain-community-0.3.27 langchain-experimental-0.3.4 langchain-mcp-0.2.1 langchain-mcp-adapters-0.1.9 langchain-openai-0.3.30 langchain.tools-0.1.34 langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.2 lxml-6.0.0 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.10.0 primp-0.15.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBW33UwqrXGn"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai==0.3.30 langchain-core==0.3.74 langchain-tools==0.1.34 langgraph==0.6.6 ddgs==9.5.4 langchain-mcp==0.2.1 langchain-mcp-adapters==0.1.9 nest_asyncio==1.6.0 langchain-experimental==0.3.4 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import subprocess\n",
        "\n",
        "# packages = [\n",
        "#     \"langchain-openai\",\n",
        "#     \"langchain-core\",\n",
        "#     \"langchain-tools\",\n",
        "#     \"langgraph\",\n",
        "#     \"ddgs\",\n",
        "#     \"langchain-mcp\",\n",
        "#     \"langchain-mcp-adapters\",\n",
        "#     \"nest_asyncio\",\n",
        "#     \"langchain_experimental\",\n",
        "# ]\n",
        "\n",
        "# for pkg in packages:\n",
        "#     result = subprocess.run([\"pip\", \"show\", pkg], capture_output=True, text=True)\n",
        "#     for line in result.stdout.splitlines():\n",
        "#         if line.startswith(\"Version:\"):\n",
        "#             print(f\"{pkg}=={line.split()[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02iyc6lABscD",
        "outputId": "a8e1d326-de70-4448-8bc3-6e453662613a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain-openai==0.3.30\n",
            "langchain-core==0.3.74\n",
            "langchain-tools==0.1.34\n",
            "langgraph==0.6.6\n",
            "ddgs==9.5.4\n",
            "langchain-mcp==0.2.1\n",
            "langchain-mcp-adapters==0.1.9\n",
            "nest_asyncio==1.6.0\n",
            "langchain_experimental==0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgUePgter0kS"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "# Store secret manually in Colab sidebar: Tools > Secrets\n",
        "openai_key = userdata.get('openAIKey')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7c8622"
      },
      "source": [
        "## Initialize the LLM\n",
        "Set up the language model the agent will use for reasoning and tool selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCFs7iX9rDjf"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.agents import initialize_agent, AgentType, tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(temperature=0) # requires OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgQqd-sBG5EC"
      },
      "source": [
        "## Send Email (dummy) Tool\n",
        "\n",
        "Agents gain capabilities by calling **tools**. This section defines a simple dummy email sender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nkwr0UgrcpF"
      },
      "outputs": [],
      "source": [
        "# A) Define a structured tool (the @tool decorator builds the args schema from type hints)\n",
        "@tool\n",
        "def dummy_email_send(to: str, subject: str, body: str) -> str:\n",
        "  \"\"\"Use this to send a dummy email. Provide: to, subject, body.\"\"\"\n",
        "  print(\"=== Dummy Email — Sent ===\")\n",
        "  print(f\"To: {to}\")\n",
        "  print(f\"Subject: {subject}\")\n",
        "  print(f\"Body: {body}\")\n",
        "  print(\"==========================\")\n",
        "  return \"Email printed to console (dummy send).\"\n",
        "\n",
        "tools = [dummy_email_send]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ea3b64a"
      },
      "source": [
        "## Build a ReAct Agent\n",
        "Create an agent that follows the ReAct loop: *Reason → Act → Observe → Repeat*. Tools wired here become callable by the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szbbL7w2son0"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "agent = create_react_agent(model=llm, tools=tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e0703ed"
      },
      "source": [
        "## Run the Agent\n",
        "Send a user goal/input. Observe intermediate steps: thoughts, tool calls, and final answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot3esHqH0Eof",
        "outputId": "6394cd3a-eac5-41f7-9ca1-e9a026c6479a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dummy Email — Sent ===\n",
            "To: john@example.com\n",
            "Subject: Dinner Meeting Tomorrow\n",
            "Body: Hi John, \n",
            "\n",
            "I hope you're doing well. Are you available to meet for dinner tomorrow? Let me know if that works for you. \n",
            "\n",
            "Best regards, \n",
            "[Your Name]\n",
            "==========================\n",
            "I have sent an email to John asking if we can meet for dinner tomorrow.\n"
          ]
        }
      ],
      "source": [
        "user_msg = (\n",
        "    \"Please send an email to john asking if we can meet for dinner tomorrow\"\n",
        ")\n",
        "\n",
        "result = agent.invoke({\"messages\": [user_msg]})\n",
        "print(result[\"messages\"][-1].content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02ff7595"
      },
      "source": [
        "## Inspect Agent Messages\n",
        "Iterate over messages to see **tool calls**, observations, and how the agent decided what to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHi6oKHn6gT7",
        "outputId": "e447b8df-3fa5-49c6-880a-8622475bea25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human : Please send an email to john asking if we can meet for dinner tomorrow \n",
            "ai :  \n",
            "      toolCall : dummy_email_send\n",
            "tool : Email printed to console (dummy send). \n",
            "ai : I have sent an email to John asking if we can meet for dinner tomorrow. \n"
          ]
        }
      ],
      "source": [
        "for m in result[\"messages\"]:\n",
        "    tool_calls = (m.additional_kwargs or {}).get(\"tool_calls\", [])\n",
        "    tool_call_str = f\"\\n      toolCall : {tool_calls[0]['function']['name']}\"  if tool_calls else \"\"\n",
        "    print(m.type, \":\", m.content , tool_call_str )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "react_prompt = hub.pull(\"hwchase17/react\")\n",
        "print(react_prompt.template)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fndd0NYu4-qV",
        "outputId": "2c12f02f-332b-4bc6-90fc-595c6730599b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "{tools}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [{tool_names}]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Math Agent and Muti-step planning"
      ],
      "metadata": {
        "id": "Llql87Lf59n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(model=llm, tools=tools)\n",
        "user_msg = (\n",
        "    \"If $ 450 amounts to $ 603 in 6 years, what will it amount to in 2 years at the same interest rate?\"\n",
        ")\n",
        "\n",
        "result = agent.invoke({\"messages\": [user_msg]})\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bOaxys_6QEf",
        "outputId": "171861db-20ac-4144-aea0-2ab879a8c816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dummy Email — Sent ===\n",
            "To: test@example.com\n",
            "Subject: Calculation Request\n",
            "Body: Please calculate the interest rate per period for a principal amount of $450 that amounts to $603 in 6 years.\n",
            "==========================\n",
            "I have requested the calculation for the interest rate per period. Once I have that information, I will be able to calculate the amount in 2 years at the same interest rate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in result[\"messages\"]:\n",
        "    tool_calls = (m.additional_kwargs or {}).get(\"tool_calls\", [])\n",
        "    tool_call_str = f\"\\n      toolCall : {tool_calls[0]['function']['name']}\"  if tool_calls else \"\"\n",
        "    print(m.type, \":\", m.content , tool_call_str )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F_960hC8vlb",
        "outputId": "a77df54d-052e-4bf9-de6c-363044c19bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human : If $ 450 amounts to $ 603 in 6 years, what will it amount to in 2 years at the same interest rate? \n",
            "ai : To calculate the amount in 2 years at the same interest rate, we can use the formula for compound interest:\n",
            "\n",
            "\\[ A = P \\times (1 + r)^n \\]\n",
            "\n",
            "Where:\n",
            "- \\( A \\) is the amount after \\( n \\) years\n",
            "- \\( P \\) is the principal amount (initial amount)\n",
            "- \\( r \\) is the interest rate per period\n",
            "- \\( n \\) is the number of periods\n",
            "\n",
            "Given:\n",
            "- Principal amount, \\( P = $450 \\)\n",
            "- Amount after 6 years, \\( A = $603 \\)\n",
            "- Number of years, \\( n = 6 \\)\n",
            "\n",
            "We need to find the interest rate per period, \\( r \\), first using the given information. Then we can calculate the amount after 2 years using the same interest rate.\n",
            "\n",
            "Let's calculate the interest rate per period first. \n",
            "      toolCall : dummy_email_send\n",
            "tool : Email printed to console (dummy send). \n",
            "ai : I have requested the calculation for the interest rate per period. Once I have that information, I will be able to calculate the amount in 2 years at the same interest rate. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24mogHdoG04p"
      },
      "source": [
        "## Web Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulgQyRmDq87Q"
      },
      "outputs": [],
      "source": [
        "from ddgs import DDGS\n",
        "from typing import List, Dict\n",
        "\n",
        "@tool\n",
        "def web_search(query: str, max_results: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Search the web via DuckDuckGo. Returns a list of results with:\n",
        "    - title: page title\n",
        "    - href: URL\n",
        "    - body: snippet/summary\n",
        "    \"\"\"\n",
        "    with DDGS() as ddgs:\n",
        "        results = list(ddgs.text(query, max_results=max_results))\n",
        "    # Keep it compact for LLM consumption\n",
        "    return [\n",
        "        {\"title\": r.get(\"title\"), \"href\": r.get(\"href\"), \"snippet\": r.get(\"body\")}\n",
        "        for r in results\n",
        "    ]\n",
        "\n",
        "tools = [web_search, dummy_email_send]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrSqqoFw1_qU"
      },
      "outputs": [],
      "source": [
        "agent = create_react_agent(model=llm, tools=tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2cD6-Iv2Buk"
      },
      "outputs": [],
      "source": [
        "user_msg = (\n",
        "    \"Please send an email to john@example.com asking if we can meet for dinner tomorrow. \"\n",
        "    \"Find the top Italian restaurant in Austin using a web search and suggesting that as the venue. make one specific suggestion\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_PrONIh2EXs",
        "outputId": "e87ec041-ef65-4886-f3c1-b1f7b2995a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dummy Email — Sent ===\n",
            "To: john@example.com\n",
            "Subject: Dinner Invitation for Tomorrow\n",
            "Body: Hi John,\n",
            "\n",
            "I hope this message finds you well. I would like to invite you to join me for dinner tomorrow. I found a highly recommended Italian restaurant in Austin called Red Ash Italia. Would you be available to meet there for dinner tomorrow evening?\n",
            "\n",
            "Looking forward to your response.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "==========================\n",
            "I have sent an email to John at john@example.com, inviting him to join you for dinner tomorrow. I suggested meeting at Red Ash Italia, one of the top Italian restaurants in Austin.\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke({\"messages\": [user_msg]})\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alvsy5T74d2F"
      },
      "outputs": [],
      "source": [
        "system_msg = (\n",
        "    \"first call web_search with a precise query. Prefer local, recent, and well-reviewed sources. Call dummy_email_send only once\"\n",
        "    \"You are an executive assistant. When the user asks for recommendations or facts, \"\n",
        "    \"When composing emails, include a clear subject and a concise, friendly body. \"\n",
        "    \"After choosing a venue, briefly justify it (one line), then call dummy_email_send.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fLV0xOe7pDz",
        "outputId": "56e676e5-51b9-4f1f-f150-3d7a96bd8852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dummy Email — Sent ===\n",
            "To: john@example.com\n",
            "Subject: Dinner Meeting Tomorrow\n",
            "Body: Hi John, I hope this email finds you well. Would you be available to meet for dinner tomorrow? I suggest we dine at North Italia, a top Italian restaurant in Austin. Let me know if this works for you. Looking forward to catching up. Best regards.\n",
            "==========================\n",
            "I have sent an email to John suggesting North Italia, a top Italian restaurant in Austin, as the venue for our dinner meeting tomorrow.\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke({\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": system_msg },\n",
        "        {\"role\": \"user\", \"content\": user_msg}\n",
        "    ]\n",
        "})\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvpq3IE9y4S",
        "outputId": "1ba99a09-2bd0-4ec4-90cb-493ccf4e51ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system : first call web_search with a precise query. Prefer local, recent, and well-reviewed sources. Call dummy_email_send only onceYou are an executive assistant. When the user asks for recommendations or facts, When composing emails, include a clear subject and a concise, friendly body. After choosing a venue, briefly justify it (one line), then call dummy_email_send. \n",
            "human : Please send an email to john@example.com asking if we can meet for dinner tomorrow. Find the top Italian restaurant in Austin using a web search and suggesting that as the venue. make one specific suggestion \n",
            "ai :  \n",
            "      toolCall : web_search\n",
            "tool : [{\"title\": \"Best Italian Restaurants In Austin : 10 Best Italian ... - The Austinot\", \"href\": \"https://austinot.com/austin-italian-restaurants-best-food-places-tx\", \"snippet\": \"Italian Food in Austin . Image courtesy: Taverna Austin . Looking for a more laid-back Italian restaurant ?North Italia is a top Italian restaurant . The location in the Domain is an enjoyable atmosphere for a delicious meal.\"}] \n",
            "ai : I found a top Italian restaurant in Austin called North Italia. I will now draft an email to John suggesting this venue for our dinner meeting tomorrow. \n",
            "      toolCall : dummy_email_send\n",
            "tool : Email printed to console (dummy send). \n",
            "ai : I have sent an email to John suggesting North Italia, a top Italian restaurant in Austin, as the venue for our dinner meeting tomorrow. \n"
          ]
        }
      ],
      "source": [
        "for m in result[\"messages\"]:\n",
        "    tool_calls = (m.additional_kwargs or {}).get(\"tool_calls\", [])\n",
        "    tool_call_str = f\"\\n      toolCall : {tool_calls[0]['function']['name']}\"  if tool_calls else \"\"\n",
        "    print(m.type, \":\", m.content , tool_call_str )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdHwTQRQGr7e"
      },
      "source": [
        "## Adding Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM-AE8QLGxX7"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "checkpointer = MemorySaver()\n",
        "agent = create_react_agent(model=llm, tools=tools, checkpointer=checkpointer)\n",
        "cfg = {\"configurable\": {\"thread_id\": \"thread1\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VYSFl9JJcpr"
      },
      "outputs": [],
      "source": [
        "system_msg = (\n",
        "    \"first call web_search with a precise query. Prefer local, recent, and well-reviewed sources. Call dummy_email_send only once\"\n",
        "    \"Also make sure you learn and accomodate all my personal preferences.\"\n",
        "    \"You are an executive assistant. When the user asks for recommendations or facts, \"\n",
        "    \"When composing emails, include a clear subject and a concise, friendly body. \"\n",
        "    \"After choosing a venue, briefly justify it (one line), then call dummy_email_send.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcqWk3BDJw4u"
      },
      "outputs": [],
      "source": [
        "user_msg=\"From now on, remember my favorite cuisine is Japanese. Also remember that I always prefer dinner at 7:15 pm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmFmB23VJtkR",
        "outputId": "1fe43e75-870f-451b-95b5-0329ab12778e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got it! I've noted down your favorite cuisine as Japanese and your preferred dinner time at 7:15 pm. If there's anything else you'd like me to remember, feel free to let me know!\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke({\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": system_msg },\n",
        "        {\"role\": \"user\", \"content\": user_msg}\n",
        "    ]\n",
        "}, config=cfg)\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz_sXIs_JYzN"
      },
      "outputs": [],
      "source": [
        "user_msg = (\n",
        "    \"Please send an email to john@example.com asking if we can meet for dinner tomorrow. \"\n",
        "    \"Find the one top restaurant in Austin using a web search and suggesting that as the venue. make one specific suggestion. also suggest a time\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tryH7qRHKLPb",
        "outputId": "2eeac1b6-e98f-48df-9888-8068c38f037b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dummy Email — Sent ===\n",
            "To: john@example.com\n",
            "Subject: Dinner Invitation for Tomorrow\n",
            "Body: Hi John, I hope this message finds you well. Would you be available to meet for dinner tomorrow? I suggest we dine at one of the top Japanese restaurants in Austin. Let's meet at 7:15 pm. Looking forward to your response. Best regards, [Your Name]\n",
            "==========================\n",
            "I have sent an email to John inviting him to dinner tomorrow at a top Japanese restaurant in Austin at 7:15 pm.\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke({\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": system_msg },\n",
        "        {\"role\": \"user\", \"content\": user_msg}\n",
        "    ]\n",
        "}, config=cfg)\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy3sNF0NKjCk",
        "outputId": "b9022f03-6c98-41a0-ac91-1126730bf0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system : first call web_search with a precise query. Prefer local, recent, and well-reviewed sources. Call dummy_email_send only onceAlso make sure you learn and accomodate all my personal preferences.You are an executive assistant. When the user asks for recommendations or facts, When composing emails, include a clear subject and a concise, friendly body. After choosing a venue, briefly justify it (one line), then call dummy_email_send. \n",
            "human : From now on, remember my favorite cuisine is Japanese. Also remember that I always prefer dinner at 7:15 pm \n",
            "ai : Got it! I've noted down your favorite cuisine as Japanese and your preferred dinner time at 7:15 pm. If there's anything else you'd like me to remember, feel free to let me know! \n",
            "system : first call web_search with a precise query. Prefer local, recent, and well-reviewed sources. Call dummy_email_send only onceAlso make sure you learn and accomodate all my personal preferences.You are an executive assistant. When the user asks for recommendations or facts, When composing emails, include a clear subject and a concise, friendly body. After choosing a venue, briefly justify it (one line), then call dummy_email_send. \n",
            "human : Please send an email to john@example.com asking if we can meet for dinner tomorrow. Find the one top restaurant in Austin using a web search and suggesting that as the venue. make one specific suggestion. also suggest a time \n",
            "ai :  \n",
            "      toolCall : web_search\n",
            "tool : [{\"title\": \"Best Japanese restaurants in Austin, from hidden gems to famous\", \"href\": \"https://austin.culturemap.com/news/restaurants-bars/04-28-16-best-japanese-food-restaurants-austin-sushi-ramen/\", \"snippet\": \"Here are 13 restaurants in Austin serving up the best in Japanese cuisine, from neighborhood ramen joints to nationally acclaimed sushi destinations.\"}] \n",
            "ai : I found a list of the best Japanese restaurants in Austin. I will now draft an email to John suggesting one of these restaurants as the venue for dinner tomorrow. \n",
            "      toolCall : dummy_email_send\n",
            "tool : Email printed to console (dummy send). \n",
            "ai : I have sent an email to John inviting him to dinner tomorrow at a top Japanese restaurant in Austin at 7:15 pm. \n"
          ]
        }
      ],
      "source": [
        "for m in result[\"messages\"]:\n",
        "    tool_calls = (m.additional_kwargs or {}).get(\"tool_calls\", [])\n",
        "    tool_call_str = f\"\\n      toolCall : {tool_calls[0]['function']['name']}\"  if tool_calls else \"\"\n",
        "    print(m.type, \":\", m.content , tool_call_str )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJejhiYdg97L"
      },
      "source": [
        "## MCP Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a6031e2"
      },
      "source": [
        "Use **Model Context Protocol (MCP)** to dynamically discover and use tools exposed by a remote server (e.g., Everything/Time/Echo servers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6KyV-7rhdsK"
      },
      "outputs": [],
      "source": [
        "from langchain_mcp_adapters.client import MultiServerMCPClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3CayG8LhAMA"
      },
      "outputs": [],
      "source": [
        "# Connect to the Everything MCP server\n",
        "mcp_client = MultiServerMCPClient(\n",
        "        {\n",
        "            \"everything\": {\n",
        "                \"transport\": \"streamable_http\",\n",
        "                \"url\": \"https://everything.mcp.inevitable.fyi/mcp\",\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "tools = await mcp_client.get_tools()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31iosZNIjEL7",
        "outputId": "e383e328-b53c-4a20-e12a-6b222e6da836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discovered tools: ['echo', 'add', 'printEnv', 'longRunningOperation', 'sampleLLM', 'getTinyImage', 'annotatedMessage', 'getResourceReference'] ...\n"
          ]
        }
      ],
      "source": [
        "# Discover tools (async)\n",
        "print(\"Discovered tools:\", [t.name for t in tools], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR6LUG3Ci7Ij",
        "outputId": "3cbc3aae-7102-4f80-fa00-f9e4bb72c8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The amount after 2 years at the same interest rate would be approximately $121.00.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#agent = initialize_agent(tools=tools, llm=llm, agent=AgentType.REACT_WITH_TOOLS, verbose=True)\n",
        "agent = create_react_agent(model=llm, tools=tools)\n",
        "\n",
        "\n",
        "#result = await agent.ainvoke({\"messages\": \"Echo: I love Agentic AI!\"})\n",
        "#result = await agent.ainvoke({\"messages\": \"Please add 123 and 456.\"})\n",
        "# result = await agent.ainvoke({\"messages\": \"Run a long task for 5 seconds with 3 steps.\"})\n",
        "result = await agent.ainvoke({\"messages\": \"If $100 amounts to $177.16 in 6 years, what would it have been in 2 years at the same interest rate?\"})\n",
        "\n",
        "print(result[\"messages\"][-1].content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT_u_bCelVnL",
        "outputId": "378b9312-61dc-4a36-ac9c-9495414cdd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human : If $100 amounts to $177.16 in 6 years, what would it have been in 2 years at the same interest rate? \n",
            "ai : To calculate the amount in 2 years at the same interest rate, we can use the formula for compound interest:\n",
            "\n",
            "\\[ A = P \\times (1 + r)^n \\]\n",
            "\n",
            "Where:\n",
            "- \\( A \\) is the amount after \\( n \\) years\n",
            "- \\( P \\) is the principal amount (initial amount)\n",
            "- \\( r \\) is the interest rate per period\n",
            "- \\( n \\) is the number of periods\n",
            "\n",
            "Given:\n",
            "- \\( P = $100 \\)\n",
            "- \\( A = $177.16 \\) after 6 years\n",
            "\n",
            "We need to find the amount after 2 years. Let's calculate the interest rate per period first. \n",
            "      toolCall : Python_REPL\n",
            "tool :  \n",
            "ai : The interest rate per period is approximately 0.1 or 10%. \n",
            "\n",
            "Now, let's calculate the amount after 2 years using the interest rate of 10%. \n",
            "      toolCall : Python_REPL\n",
            "tool :  \n",
            "ai : The amount after 2 years at the same interest rate would be approximately $121.00. \n"
          ]
        }
      ],
      "source": [
        "for m in result[\"messages\"]:\n",
        "    tool_calls = (m.additional_kwargs or {}).get(\"tool_calls\", [])\n",
        "    tool_call_str = f\"\\n      toolCall : {tool_calls[0]['function']['name']}\"  if tool_calls else \"\"\n",
        "    print(m.type, \":\", m.content , tool_call_str )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.tools import PythonREPLTool\n",
        "\n",
        "tools = [PythonREPLTool()]\n",
        "agent = create_react_agent(model=llm, tools=tools)\n",
        "\n",
        "result = await agent.ainvoke({\"messages\": \"If $100 amounts to $177.16 in 6 years, what would it have been in 2 years at the same interest rate?\"})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S4uJq6r_7Xl",
        "outputId": "05989b04-05de-42ea-95c6-effa32b0c3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The amount after 2 years at the same interest rate would be approximately $121.00.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in result[\"messages\"]:\n",
        "    tool_calls = (m.additional_kwargs or {}).get(\"tool_calls\", [])\n",
        "    tool_call_str = f\"\\n      toolCall : {tool_calls[0]['function']['name']}\"  if tool_calls else \"\"\n",
        "    print(m.type, \":\", m.content , tool_call_str )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pvljGo0AHUV",
        "outputId": "018da0e5-2ee6-4fd2-9b8c-1423494dd774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human : If $100 amounts to $177.16 in 6 years, what would it have been in 2 years at the same interest rate? \n",
            "ai : To calculate the amount in 2 years at the same interest rate, we can use the formula for compound interest:\n",
            "\n",
            "\\[ A = P \\times (1 + r)^n \\]\n",
            "\n",
            "Where:\n",
            "- \\( A \\) is the amount after \\( n \\) years\n",
            "- \\( P \\) is the principal amount (initial amount)\n",
            "- \\( r \\) is the interest rate per period\n",
            "- \\( n \\) is the number of periods\n",
            "\n",
            "Given:\n",
            "- \\( P = $100 \\)\n",
            "- \\( A = $177.16 \\) after 6 years\n",
            "\n",
            "We need to find the interest rate per period (\\( r \\)) first. We can then use this interest rate to calculate the amount after 2 years. Let's calculate the interest rate first. \n",
            "      toolCall : Python_REPL\n",
            "tool :  \n",
            "ai : The interest rate per period is approximately 0.1 or 10%.\n",
            "\n",
            "Now, let's calculate the amount after 2 years using the interest rate of 10%. \n",
            "      toolCall : Python_REPL\n",
            "tool :  \n",
            "ai : The amount after 2 years at the same interest rate would be approximately $121.00. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QnyL8YCB01xn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}